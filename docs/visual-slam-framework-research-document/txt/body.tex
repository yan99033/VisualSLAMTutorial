A visual SLAM framework provides the foundation for integrating different components within it. 
For example, A visual SLAM system consists of camera tracking, mapping, loop closing, and visualization components. The framework connects the components such that we get the camera motion and the structure of the environment from a stream of images in real-time.


\section{Problem}

The outcome of this document is twofold: 
First, we want to find a \textit{good} way to connect the visual SLAM components. 
Second, while achieving the first outcome, we also define the structure of a repository in which the code is stored.
As a general rule of thumb, the \textit{good}ness of the framework is prioritized on the ease-of-use while maintaining the real-time capability.
Specifically, each component in the framework can be defined as a module, which can be customized or replaced for enhanced performance, so long as the interface (the inputs and outputs) is unchanged.

\section{Literature review}

A good visual SLAM framework allows for integrating the camera tracking, mapping, loop closing and visualization components and the intercommunication between them.
A good starting point is to study the self-driving car or autopilot projects on GitHub, notably \href{https://github.com/ApolloAuto/apollo}{Apollo by Baidu} and \href{https://gitlab.com/autowarefoundation/autoware.auto/AutowareAuto}{Autoware.Auto}\footnote{New versions of Autoware are under development, which can be found \href{https://github.com/autowarefoundation/autoware}{here}}.
They use their own middleware (\href{https://cyber-rt.readthedocs.io/en/latest/index.html}{Cyber RT}) and \href{https://docs.ros.org/en/humble/}{ROS2}. 
Wu et al.~\ref{wu2021oops} show that ROS2 is better than Cyber RT in terms of latency and CPU usage and recommend the use of shared memory (specifically intra-process (IAP) in ROS2) over data serialization/deserialization for the best latency and reliability.
Additionally, ROS2 is commonly adopted in robotics projects, with the support of new sensor integration. 

Among the existing visual SLAM systems, ORB-SLAM3 is one of the most practical and open-source visual SLAM system.
The practicality comes from the well-engineered algorithms designed to handle edge cases like improved camera tracking through multi-map merging and robust loop closure detection.

\section{Discussion}
To promote flexibility, we can organize the visual SLAM components as individual modules/libraries so that we can test them in isolation before integrating into the framework. As

\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\textwidth]{img/pipeline.png}
    \caption{Visual SLAM pipeline}
    \label{fig:vslam_pipeline}
\end{figure}

Figure~\ref{fig:vslam_pipeline} shows the proposed visual SLAM pipeline with the components implemented as individual composable nodes. 
The role of each node is described as follows:
\begin{itemize}
  \item \textbf{data\_loader\_node} takes in a list of images from a folder and convert them into the Frame message, which contains the image and the camera information.
  
  \item \textbf{feature\_extraction\_node} calculates the features (e.g., corners or high-gradient image regions) in the image.
  
  \item \textbf{feature\_matching\_node} finds feature correspondences between the current frame and the nearest keyframe in the back-end.
  If the system is in the initialization state, the current frame will be set as the tentative keyframe.  
  If the system is in the relocalization mode, the current keyframe 

  \item \textbf{tracking\_node}  

  \item \textbf{mapping\_node} calculates the current camera pose relative to the current keyframe. If there is no keyframe, it initiates the initialization step, which performs 2D-2D camera tracking and request for a new map. 

\end{itemize}

\subsection{Interfaces}


% * https://github.com/ZhenshengLee/ros2_shm_msgs/blob/humble/doc/Using_Zero_Copy_In_ROS2.pdf
% Octree to visualize millions of points? (https://github.com/heremaps/pptk)

\section{Appendix}

\subsection{Custom interfaces}

\subsubsection{Messages}

\begin{itemize}
  \item \textbf{vslam\_msgs/Frame}
  \begin{verbatim}
    # Header
    std_msgs/Header header

    # Camera information
    sensor_msgs/CameraInfo cam_info

    # Frame ID
    uint32 id 

    # Image
    sensor_msgs/Image image
    
    # Camera pose
    geometry_msgs/Pose pose
    
    # List of points and their correspondences in another frame
    vslam_msgs/Point [] pts
    vslam_msgs/MatchedPoint [] matches
  \end{verbatim}

  \item \textbf{vslam\_msgs/MatchedPoint}
  \begin{verbatim}
    vslam_msgs/Point pt1    
    vslam_msgs/Point pt2
  \end{verbatim}

  \item \textbf{vslam\_msgs/Vector3d}
  \begin{verbatim}
    float64 x
    float64 y  
    float64 z 
  \end{verbatim}

  \item \textbf{vslam\_msgs/Vector2d}
  \begin{verbatim}
    float64 x 
    float64 y 
  \end{verbatim}

  \item \textbf{vslam\_msgs/Point}
  \begin{verbatim}
    uint32 frame_id
    
    # Keypoint
    vslam_msgs/Vector2d kp

    # Map point
    vslam_msgs/Vector3d mp
    bool has_mp = false
    
    # Feature
    vslam_msgs/Feature feature    
  \end{verbatim}

  \item \textbf{vslam\_msgs/Feature}
  \begin{verbatim}
    # The length and data structure of the descriptor to be 
    #   defined in the code
    uint32 len
    uint8 [] descriptor
  \end{verbatim}

  \item \textbf{vslam\_msgs/State}
  \begin{verbatim}
    uint8 INITIALIZATION = 0
    uint8 ATTEMPT_INITIALIZATION = 1
    uint8 TRACKING = 2
    uint8 RELOCALIZATION = 3
    
    uint8 state = 0
  \end{verbatim}
\end{itemize}

\subsubsection{Services}

\begin{itemize}
  \item \textbf{vslam\_msgs/GetKeyFrame}
  \begin{verbatim}
    # Get the keyframe with the provided frame id
    # Default to -1, which returns the current keyframe
    # In relocalization mode, place recognition node may 
    #   returns the frame id which can be used retrieve 
    #   the candidate keyframe for relocalization
    int64 frame_id = -1
    ---
    vslam_msgs/Frame keyframe
    
    # Return true if a keyframe is found
    # Could be false if the tracking is lost and there is no 
    #   similar-looking image found
    bool has_keyframe
  \end{verbatim}

  \item \textbf{vslam\_msgs/SetKeyFrame}
  \begin{verbatim}
    vslam_msgs/Frame keyframe
    ---
    # Indicate if the keyframe is set
    bool success
  \end{verbatim}

  \item \textbf{vslam\_msgs/GetState}
  \begin{verbatim}
    ---
    # Get the state of the system 
    #   (e.g., initialization, tracking or relocalization)
    vslam_msgs/State state
  \end{verbatim}

  \item \textbf{vslam\_msgs/SetState}
  \begin{verbatim}
    # Set the state of the system 
    #   (e.g., initialization, tracking or relocalization)
    vslam_msgs/State state
    ---
    # Indicate if the state is set
    bool success
    
  \end{verbatim}
\end{itemize}











\subsection{ROS2 study}

To ensure ROS2 is the right fit for the visual SLAM framework, we need to experiment with the modularity and the memory-sharing mechanism provided by it.
\href{https://github.com/ros-planning/navigation2}{Nav2} provides examples of creating action servers (e.g., planner and controller), each of which could have several plugins to accomplish various tasks.
The action servers are ROS2 nodes managed by their \href{https://github.com/ros2/demos/blob/humble/lifecycle/README.rst}{lifecycle manager} to activate or deactivate the nodes on demand.
The nodes are loaded as \href{https://docs.ros.org/en/humble/How-To-Guides/Launching-composable-nodes.html}{composable nodes} to allow for \href{https://docs.ros.org/en/humble/Tutorials/Demos/Intra-Process-Communication.html}{intra-communication} among the nodes.
Therefore, we could use action servers to represent the core components in the visual SLAM system and potentially use intra-process communication to reduce the overhead from unnecessary data copy between nodes.